{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "## import sklearn metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "## import sklearn models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>120.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0542</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  SibSp  Parch      Fare  Embarked_C  \\\n",
       "0         0       3    0  14.0      0      0    7.8542           0   \n",
       "1         1       1    1  28.0      0      0   26.5500           0   \n",
       "2         1       1    0  36.0      1      2  120.0000           0   \n",
       "3         0       3    1  17.0      1      0    7.0542           0   \n",
       "4         0       3    1   4.0      4      2   31.2750           0   \n",
       "\n",
       "   Embarked_Q  Embarked_S  \n",
       "0           0           1  \n",
       "1           0           1  \n",
       "2           0           1  \n",
       "3           0           1  \n",
       "4           0           1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import clean data\n",
    "titanic_df = pd.read_csv('datasets/titanic_processed.csv')\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pclass',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Fare',\n",
       " 'Embarked_C',\n",
       " 'Embarked_Q',\n",
       " 'Embarked_S']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## since survived is col[0] then all of our features are col[1:]\n",
    "FEATURES = list(titanic_df.columns[1:])\n",
    "FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will store the results for our model metrics here\n",
    "result_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_classification(y_test, y_pred):\n",
    "    acc = accuracy_score(y_test, y_pred, normalize=True)\n",
    "    num_acc = accuracy_score(y_test, y_pred, normalize=False)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    return {'accuracy': acc,\n",
    "           'precision': prec,\n",
    "           'recall': recall,\n",
    "           'accuracy_count':num_acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model (classifier_fn,\n",
    "                name_of_y_col,\n",
    "                names_of_x_cols,\n",
    "                dataset,\n",
    "                test_frac=0.2):\n",
    "    # get features and labels\n",
    "    X = dataset[names_of_x_cols]\n",
    "    Y = dataset[name_of_y_col]\n",
    "    # instantiate predictor\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = test_frac)\n",
    "    # train model\n",
    "    model = classifier_fn(x_train, y_train)\n",
    "    # test model\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    \n",
    "    train_summary = summarize_classification(y_train, y_pred_train)\n",
    "    test_summary = summarize_classification(y_test, y_pred)\n",
    "    \n",
    "    ## objects for displaying results\n",
    "    pred_results = pd.DataFrame({'y_test':y_test,'y_pred':y_pred})\n",
    "    model_crosstab = pd.crosstab(pred_results.y_pred, pred_results.y_test)\n",
    "    \n",
    "    return {'training':train_summary,\n",
    "           'test': test_summary,\n",
    "           'confusion_matrix':model_crosstab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results():\n",
    "    for key in result_dict:\n",
    "        print('Classification: ',key)\n",
    "        print()\n",
    "        print('Training data')\n",
    "        for score in result_dict[key]['training']:\n",
    "            print(score, result_dict[key]['training'][score])\n",
    "        print()\n",
    "        print('Test data')\n",
    "        for score in result_dict[key]['test']:\n",
    "            print(score,result_dict[key]['test'][score])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_fn(x_train, y_train):\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(x_train,y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['survived - logistic'] = build_model(logistic_fn,\n",
    "                                                 'Survived',\n",
    "                                                 FEATURES,\n",
    "                                                 titanic_df)\n",
    "#compare_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LDA, QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "## the lineardiscriminantanalysis tries to reduce demensionality based on the axis that best seperates data into different classes\n",
    "## the svd solver tries to find this without calculating the covariance matrix of features (usefull when you have many features or rows of data)\n",
    "def linear_discriminant_fn(x_train, y_train, solver='svd'):\n",
    "    model = LinearDiscriminantAnalysis(solver=solver)\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "result_dict['survived - linear_discriminant_analysis'] = build_model(linear_discriminant_fn,\n",
    "                                                 'Survived',\n",
    "                                                 FEATURES,\n",
    "                                                 titanic_df)\n",
    "# compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## when we one-hot encoded the data, we introduced colinear variables this is called a dummy trap.\n",
    "## Logistic regression removes this issue before it runs but not all estimators do this\n",
    "## we can remove on of the dummie columns to remove the collinear\n",
    "## instead of one-hot encoding we should use dummy encoding next time\n",
    "def linear_discriminant_fn(x_train, y_train, solver='svd'):\n",
    "    model = LinearDiscriminantAnalysis(solver=solver)\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "result_dict['survived - linear_discriminant_analysis'] = build_model(linear_discriminant_fn,\n",
    "                                                 'Survived',\n",
    "                                                 FEATURES[0:-1], ## this is where we drop one of the dummy collumns\n",
    "                                                 titanic_df)\n",
    "# compare_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use qda when your x variables corresponding to different labels have different covariances\n",
    "## i.e covariances are different for X for all values of Y\n",
    "def quadratic_discriminant_fn(x_train, y_train):\n",
    "    model = QuadraticDiscriminantAnalysis()\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "result_dict['survived - quadratic_discriminant_analysis'] = build_model(quadratic_discriminant_fn,\n",
    "                                                 'Survived',\n",
    "                                                 FEATURES[0:-1], ## this is where we drop one of the dummy collumns\n",
    "                                                 titanic_df)\n",
    "# compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## max_iter is max iterations this model with train\n",
    "## tol is the stopping criterian for the model training, this means the slope flattening out, and\n",
    "# the model isn't changing much in training\n",
    "## sometimes a high max iteration will yield better modeling\n",
    "\n",
    "def sgd_fn (x_train, y_train, max_iter=1000, tol=1e-3):\n",
    "    model = SGDClassifier(max_iter=max_iter, tol=tol)\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "result_dict['survived - SGD'] = build_model(sgd_fn,\n",
    "                                                 'Survived',\n",
    "                                                 FEATURES, \n",
    "                                                 titanic_df)\n",
    "#compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_svc_fn(x_train, y_train, C=1.0, max_iter=1000, tol=1e-3):\n",
    "    ## you can also use SVC(kernel=\"linear\")\n",
    "    ## when the num samples > num features use dual=False\n",
    "    model = LinearSVC(C=C, max_iter=max_iter, tol=tol, dual=False)\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "result_dict['survived - LinearSVC'] = build_model(linear_svc_fn,\n",
    "                                                 'Survived',\n",
    "                                                 FEATURES, \n",
    "                                                 titanic_df)\n",
    "# compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## by default this function considers all entities within 40 units to be a neighbor\n",
    "\n",
    "def radius_neighbor_fn(x_train, y_train, radius=40.0):\n",
    "    model = RadiusNeighborsClassifier(radius=radius)\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "result_dict['survived - radius_neighbors'] = build_model(radius_neighbor_fn,\n",
    "                                                        'Survived',\n",
    "                                                        FEATURES,\n",
    "                                                        titanic_df)\n",
    "# compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_fn(x_train, y_train, max_depth=None, max_features=None):\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth, max_features=max_features)\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "result_dict['survived - decision_tree'] = build_model(decision_tree_fn,\n",
    "                                                        'Survived',\n",
    "                                                        FEATURES,\n",
    "                                                        titanic_df)\n",
    "# compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification:  survived - logistic\n",
      "\n",
      "Training data\n",
      "accuracy 0.7873462214411248\n",
      "precision 0.7623762376237624\n",
      "recall 0.6784140969162996\n",
      "accuracy_count 448\n",
      "\n",
      "Test data\n",
      "accuracy 0.8251748251748252\n",
      "precision 0.8\n",
      "recall 0.7868852459016393\n",
      "accuracy_count 118\n",
      "\n",
      "Classification:  survived - linear_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.7824074074074074\n",
      "recall 0.7161016949152542\n",
      "accuracy_count 455\n",
      "\n",
      "Test data\n",
      "accuracy 0.7762237762237763\n",
      "precision 0.7083333333333334\n",
      "recall 0.6538461538461539\n",
      "accuracy_count 111\n",
      "\n",
      "Classification:  survived - quadratic_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.8014059753954306\n",
      "precision 0.7402597402597403\n",
      "recall 0.7633928571428571\n",
      "accuracy_count 456\n",
      "\n",
      "Test data\n",
      "accuracy 0.7342657342657343\n",
      "precision 0.7321428571428571\n",
      "recall 0.640625\n",
      "accuracy_count 105\n",
      "\n",
      "Classification:  survived - SGD\n",
      "\n",
      "Training data\n",
      "accuracy 0.7117750439367311\n",
      "precision 0.753731343283582\n",
      "recall 0.4353448275862069\n",
      "accuracy_count 405\n",
      "\n",
      "Test data\n",
      "accuracy 0.7272727272727273\n",
      "precision 0.7931034482758621\n",
      "recall 0.4107142857142857\n",
      "accuracy_count 104\n",
      "\n",
      "Classification:  survived - LinearSVC\n",
      "\n",
      "Training data\n",
      "accuracy 0.7855887521968365\n",
      "precision 0.75\n",
      "recall 0.7043478260869566\n",
      "accuracy_count 447\n",
      "\n",
      "Test data\n",
      "accuracy 0.7902097902097902\n",
      "precision 0.7258064516129032\n",
      "recall 0.7758620689655172\n",
      "accuracy_count 113\n",
      "\n",
      "Classification:  survived - radius_neighbors\n",
      "\n",
      "Training data\n",
      "accuracy 0.6678383128295254\n",
      "precision 0.7362637362637363\n",
      "recall 0.28879310344827586\n",
      "accuracy_count 380\n",
      "\n",
      "Test data\n",
      "accuracy 0.6433566433566433\n",
      "precision 0.631578947368421\n",
      "recall 0.21428571428571427\n",
      "accuracy_count 92\n",
      "\n",
      "Classification:  survived - decision_tree\n",
      "\n",
      "Training data\n",
      "accuracy 0.984182776801406\n",
      "precision 1.0\n",
      "recall 0.9620253164556962\n",
      "accuracy_count 560\n",
      "\n",
      "Test data\n",
      "accuracy 0.8391608391608392\n",
      "precision 0.78\n",
      "recall 0.7647058823529411\n",
      "accuracy_count 120\n",
      "\n",
      "Classification:  survived - neive_bayes\n",
      "\n",
      "Training data\n",
      "accuracy 0.7715289982425307\n",
      "precision 0.7244444444444444\n",
      "recall 0.7056277056277056\n",
      "accuracy_count 439\n",
      "\n",
      "Test data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.8\n",
      "recall 0.7719298245614035\n",
      "accuracy_count 119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## use bayes theorem to find which label is most likely, given the attributes -\n",
    "## observed in the feature vector and given how often the different -\n",
    "## labels occur in the data\n",
    "def naive_bayes_fn(x_train, y_train, priors=None):\n",
    "    model = GaussianNB(priors=priors)\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "result_dict['survived - neive_bayes'] = build_model(naive_bayes_fn,\n",
    "                                                        'Survived',\n",
    "                                                        FEATURES,\n",
    "                                                        titanic_df)\n",
    "compare_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
